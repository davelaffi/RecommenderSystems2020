{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ybQSYdHvb_CB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "from Base.DataIO import DataIO\n",
    "\n",
    "#----Recommenders----\n",
    "from SLIM.SLIM_BPR_Python import SLIM_BPR_Python\n",
    "from SLIM.SlimElasticNet import SLIMElasticNetRecommender\n",
    "from cf.item_cf3 import ItemBasedCollaborativeFiltering\n",
    "from cf.user_cf2 import UserBasedCollaborativeFiltering\n",
    "from MF.ALS import AlternatingLeastSquare\n",
    "from cbf.cbf import ContentBasedFiltering\n",
    "from SlimBPR.SlimBPRRec import SlimBPRRec\n",
    "from SlimBPR.SlimBPR import SlimBPR\n",
    "\n",
    "from Hybrid.hybridRec import HybridRecommender\n",
    "#---------------\n",
    "\n",
    "#----Model Load & Save----\n",
    "from Data_manager.Dataset import Dataset\n",
    "#-------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyPCmUzVedBh"
   },
   "source": [
    "**Dataset loading with pandas**\n",
    "\n",
    "The function read_csv from pandas provides a wonderful and fast interface to load tabular data like this. For better results and performance we provide the separator ::, the column names [\"user_id\", \"item_id\", \"ratings\", \"timestamp\"], and the types of each attribute in the dtype parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96AUQmIlcAr2"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  return pd.read_csv(\"./data_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMaAYCCksV82"
   },
   "outputs": [],
   "source": [
    "ratings=load_data()\n",
    "d ={'user_id': ratings['row'],'item_id':ratings['col'],'ratings':ratings['data']}\n",
    "ratings=pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "enpGAp964Y5a",
    "outputId": "c436ac0a-ab96-4e87-8990-dea6db1076a0"
   },
   "outputs": [],
   "source": [
    "ratings.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dj1UlZkbiJd"
   },
   "outputs": [],
   "source": [
    "userList=list(d['user_id'])\n",
    "itemList=list(d['item_id'])\n",
    "ratingList=list(d['ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuTgqT-1biJd"
   },
   "outputs": [],
   "source": [
    "URM = sp.coo_matrix((ratingList,(userList,itemList)))\n",
    "URM = URM.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4kLvs-TbiJd",
    "outputId": "547ba2f6-8e3d-4fda-9dc3-08577d183c47"
   },
   "outputs": [],
   "source": [
    "URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FSE7ic-VbiJd"
   },
   "outputs": [],
   "source": [
    "def load_data_ICM():\n",
    "  return pd.read_csv(\"./data_ICM_title_abstract.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-d-NVkEZbiJd"
   },
   "outputs": [],
   "source": [
    "features=load_data_ICM()\n",
    "d ={'item_id': features['row'],'feature_id':features['col'],'value':features['data']}\n",
    "features=pd.DataFrame(data=d)\n",
    "itemList=list(d['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hJs6hZ8obiJd"
   },
   "outputs": [],
   "source": [
    "featureList=list(d['feature_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kR-oPdRtbiJd"
   },
   "outputs": [],
   "source": [
    "valueList=list(d['value'])\n",
    "ICM = sp.coo_matrix((valueList,(itemList,featureList)))\n",
    "ICM = ICM.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_PXL-kG4biJd",
    "outputId": "163614e6-32e7-451c-b1b2-0de16719dea1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25975x20000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 490691 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users=URM.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items=URM.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users,num_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLb4ij1thWhs"
   },
   "source": [
    "**Dataset splitting into train,validation and test**\n",
    "\n",
    "This is the last part before creating the recommender. However, this step is super important, as it is the base for the training, parameters optimization, and evaluation of the recommender(s).\n",
    "\n",
    "In here we read the ratings (which we loaded and preprocessed before) and create the train, validation, and test User-Rating Matrices (URM). It's important that these are disjoint to avoid information leakage from the train into the validation/test set, in our case, we are safe to use the train_test_split function from scikit-learn as the dataset only contains one datapoint for every (user,item) pair. On another topic, we first create the test set and then we create the validation by splitting again the train set.\n",
    "\n",
    "train_test_split takes an array (or several arrays) and divides it into train and test according to a given size (in our case testing_percentage and validation_percentage, which need to be a float between 0 and 1).\n",
    "\n",
    "After we have our different splits, we create the sparse URMs by using the csr_matrix function from scipy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Notebooks_utils.data_splitter import train_test_holdout\n",
    "\n",
    "urm_train_validation, urm_test = train_test_holdout(URM, train_perc = 0.85)\n",
    "urm_train, urm_validation = train_test_holdout(urm_train_validation, train_perc = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Data\n",
    "dataset = Dataset(\"prova\", {\"urm_train\" : urm_train}, {\"urm_validation\" : urm_validation}, {\"urm_test\" : urm_test}) \n",
    "dataset.save_data(\"Salvataggi/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load\n",
    "loaded_dataset = Dataset(None, None, None, None)\n",
    "loaded_dataset.load_data(\"Salvataggi/\")\n",
    "\n",
    "\n",
    "urm_train = (loaded_dataset.get_urm_train())[\"urm_train\"]\n",
    "urm_validation = (loaded_dataset.get_urm_validation())[\"urm_validation\"]\n",
    "urm_test = loaded_dataset.get_urm_test()[\"urm_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urm_train_validation = urm_train + urm_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(urm_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(urm_test, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "hyperparameters_range_dictionary = {}\n",
    "hyperparameters_range_dictionary[\"knn\"] = Integer(5, 1000)\n",
    "hyperparameters_range_dictionary[\"shrink\"] = Integer(0, 1000)\n",
    "hyperparameters_range_dictionary[\"similarity\"] = Categorical([\"cosine\",\n",
    "#                                                              \"pearson\" \n",
    "#                                                               \"adjusted\",\n",
    "#                                                               \"asymmetric\",\n",
    "                                                               \"jaccard\",\n",
    "#                                                               \"tanimoto\", \n",
    "#                                                               \"dice\", \n",
    "#                                                               \"tversky\"\n",
    "                                                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "hyperparameters_range_dictionary = {}\n",
    "hyperparameters_range_dictionary[\"n_factors\"] = Integer(20, 1000)\n",
    "hyperparameters_range_dictionary[\"regularization\"] = Real(0, 2)\n",
    "hyperparameters_range_dictionary[\"iterations\"] = Integer(10, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ParameterTuning.SearchBayesianSkopt import SearchBayesianSkopt\n",
    "\n",
    "recommender_class = AlternatingLeastSquare\n",
    "\n",
    "parameterSearch = SearchBayesianSkopt(recommender_class,\n",
    "                                 evaluator_validation=evaluator_validation,\n",
    "                                 evaluator_test=evaluator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ParameterTuning.SearchAbstractClass import SearchInputRecommenderArgs\n",
    "  \n",
    "recommender_input_args = SearchInputRecommenderArgs(\n",
    "    CONSTRUCTOR_POSITIONAL_ARGS = [urm_train],     # For a CBF model simply put [URM_train, ICM_train]\n",
    "    CONSTRUCTOR_KEYWORD_ARGS = {},\n",
    "    FIT_POSITIONAL_ARGS = [],\n",
    "    FIT_KEYWORD_ARGS = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_input_args_last_test = SearchInputRecommenderArgs(\n",
    "    CONSTRUCTOR_POSITIONAL_ARGS = [urm_train_validation],     # For a CBF model simply put [URM_train_validation, ICM_train]\n",
    "    CONSTRUCTOR_KEYWORD_ARGS = {},\n",
    "    FIT_POSITIONAL_ARGS = [],\n",
    "    FIT_KEYWORD_ARGS = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_folder_path = \"result_experiments/\"\n",
    "\n",
    "# If directory does not exist, create\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "    \n",
    "n_cases = 50\n",
    "n_random_starts = int(n_cases*0.3)\n",
    "metric_to_optimize = \"MAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "SearchBayesianSkopt: Testing config: {'n_factors': 138, 'regularization': 0.4450306306443862, 'iterations': 17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e725b391d3fa4c7ebf60c2f06fa653fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SearchBayesianSkopt: Config 0 Exception. Config: {'n_factors': 138, 'regularization': 0.4450306306443862, 'iterations': 17} - Exception: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchAbstractClass.py\", line 362, in _objective_function\n",
      "    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchAbstractClass.py\", line 272, in _evaluate_on_validation\n",
      "    result_dict, _ = self.evaluator_validation.evaluateRecommender(recommender_instance)\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\Base\\Evaluation\\Evaluator.py\", line 245, in evaluateRecommender\n",
      "    results_dict = self._run_evaluation_on_selected_users(recommender_object, self.users_to_evaluate)\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\Base\\Evaluation\\Evaluator.py\", line 449, in _run_evaluation_on_selected_users\n",
      "    return_scores = True\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\Base\\BaseRecommender.py\", line 78, in recommend\n",
      "    scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\Base\\BaseRecommender.py\", line 51, in _compute_item_score\n",
      "    item_scores = user_profile_array.dot(self.W_sparse).toarray()\n",
      "  File \"C:\\Users\\davel\\anaconda3\\envs\\RecSysFramework\\lib\\site-packages\\scipy\\sparse\\base.py\", line 364, in dot\n",
      "    return self * other\n",
      "  File \"C:\\Users\\davel\\anaconda3\\envs\\RecSysFramework\\lib\\site-packages\\scipy\\sparse\\base.py\", line 518, in __mul__\n",
      "    raise ValueError('dimension mismatch')\n",
      "ValueError: dimension mismatch\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchAbstractClass.py\", line 362, in _objective_function\n",
      "    result_dict, result_string, recommender_instance, train_time, evaluation_time = self._evaluate_on_validation(current_fit_parameters_dict)\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchAbstractClass.py\", line 272, in _evaluate_on_validation\n",
      "    result_dict, _ = self.evaluator_validation.evaluateRecommender(recommender_instance)\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\Base\\Evaluation\\Evaluator.py\", line 245, in evaluateRecommender\n",
      "    results_dict = self._run_evaluation_on_selected_users(recommender_object, self.users_to_evaluate)\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\Base\\Evaluation\\Evaluator.py\", line 449, in _run_evaluation_on_selected_users\n",
      "    return_scores = True\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\Base\\BaseRecommender.py\", line 78, in recommend\n",
      "    scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\n",
      "  File \"C:\\Users\\davel\\Desktop\\ChallengeRecommender\\Base\\BaseRecommender.py\", line 51, in _compute_item_score\n",
      "    item_scores = user_profile_array.dot(self.W_sparse).toarray()\n",
      "  File \"C:\\Users\\davel\\anaconda3\\envs\\RecSysFramework\\lib\\site-packages\\scipy\\sparse\\base.py\", line 364, in dot\n",
      "    return self * other\n",
      "  File \"C:\\Users\\davel\\anaconda3\\envs\\RecSysFramework\\lib\\site-packages\\scipy\\sparse\\base.py\", line 518, in __mul__\n",
      "    raise ValueError('dimension mismatch')\n",
      "ValueError: dimension mismatch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 6.5809\n",
      "Function value obtained: 65504.0000\n",
      "Current minimum: 65504.0000\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "SearchBayesianSkopt: Testing config: {'n_factors': 696, 'regularization': 1.4335949333365094, 'iterations': 107}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b59a65559024a5ba351476a7df48144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=107), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a98d30f4e389>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                        \u001b[0moutput_folder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_folder_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                        \u001b[0moutput_file_name_root\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecommender_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRECOMMENDER_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                        \u001b[0mmetric_to_optimize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric_to_optimize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                       )\n",
      "\u001b[1;32m~\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchBayesianSkopt.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, recommender_input_args, parameter_search_space, metric_to_optimize, n_cases, n_random_starts, output_folder_path, output_file_name_root, save_model, save_metadata, resume_from_saved, recommender_input_args_last_test, evaluate_on_test_each_best_solution)\u001b[0m\n\u001b[0;32m    260\u001b[0m                                   \u001b[0mkappa\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkappa\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                                   \u001b[0mnoise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                                   n_jobs=self.n_jobs)\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSysFramework\\lib\\site-packages\\skopt\\optimizer\\gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSysFramework\\lib\\site-packages\\skopt\\optimizer\\base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mnext_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchBayesianSkopt.py\u001b[0m in \u001b[0;36m_objective_function_list_input\u001b[1;34m(self, current_fit_parameters_list_of_values)\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mcurrent_fit_parameters_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparams_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_fit_parameters_list_of_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_objective_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_fit_parameters_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchAbstractClass.py\u001b[0m in \u001b[0;36m_objective_function\u001b[1;34m(self, current_fit_parameters_dict)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[1;31m# If getting a interrupt, terminate without saving the exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchAbstractClass.py\u001b[0m in \u001b[0;36m_objective_function\u001b[1;34m(self, current_fit_parameters_dict)\u001b[0m\n\u001b[0;32m    360\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hyperparameters_list\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_counter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_fit_parameters_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m             \u001b[0mresult_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecommender_instance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluation_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluate_on_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_fit_parameters_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[0mcurrent_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mresult_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric_to_optimize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchAbstractClass.py\u001b[0m in \u001b[0;36m_evaluate_on_validation\u001b[1;34m(self, current_fit_parameters)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_evaluate_on_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_fit_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mrecommender_instance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_fit_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ChallengeRecommender\\ParameterTuning\\SearchAbstractClass.py\u001b[0m in \u001b[0;36m_fit_model\u001b[1;34m(self, current_fit_parameters)\u001b[0m\n\u001b[0;32m    255\u001b[0m         recommender_instance.fit(*self.recommender_input_args.FIT_POSITIONAL_ARGS,\n\u001b[0;32m    256\u001b[0m                                  \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecommender_input_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFIT_KEYWORD_ARGS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m                                  **current_fit_parameters)\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ChallengeRecommender\\MF\\ALS.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, n_factors, regularization, iterations)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_conf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# Get the user and item vectors from our trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\RecSysFramework\\lib\\site-packages\\implicit\\als.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, item_users, show_progress)\u001b[0m\n\u001b[0;32m    173\u001b[0m                        num_threads=self.num_threads)\n\u001b[0;32m    174\u001b[0m                 solver(Ciu, self.item_factors, self.user_factors, self.regularization,\n\u001b[1;32m--> 175\u001b[1;33m                        num_threads=self.num_threads)\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameterSearch.search(recommender_input_args,\n",
    "                       recommender_input_args_last_test = recommender_input_args_last_test,\n",
    "                       parameter_search_space = hyperparameters_range_dictionary,\n",
    "                       n_cases = n_cases,\n",
    "                       n_random_starts = n_random_starts,\n",
    "                       save_model = \"last\",\n",
    "                       output_folder_path = output_folder_path,\n",
    "                       output_file_name_root = recommender_class.RECOMMENDER_NAME,\n",
    "                       metric_to_optimize = metric_to_optimize,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataIO(folder_path = output_folder_path)\n",
    "search_metadata = data_loader.load_data(recommender_class.RECOMMENDER_NAME + \"_metadata.zip\")\n",
    "best_parameters = search_metadata[\"hyperparameters_best\"]\n",
    "best_parameters\n",
    "#last: 381, 515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(recommendations: np.array, relevant_items: np.array) -> float:\n",
    "    is_relevant = np.in1d(recommendations, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "    \n",
    "    \n",
    "def precision(recommendations: np.array, relevant_items: np.array) -> float:\n",
    "    is_relevant = np.in1d(recommendations, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant) / recommendations.shape[0]\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "def mean_average_precision(recommendations: np.array, relevant_items: np.array) -> float:\n",
    "    is_relevant = np.in1d(recommendations, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "\n",
    "    map_score = np.sum(precision_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Procedure**\n",
    "\n",
    "The evaluation procedure returns the averaged accuracy scores (in terms of precision, recall and MAP) for all users (that have at least 1 rating in the test set). It also calculates the number of evaluated and skipped users. It receives a recommender instance, and the train and test URMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(recommender: object, urm_train: sp.csr_matrix, urm_test: sp.csr_matrix):\n",
    "    recommendation_length = 10\n",
    "    accum_precision = 0\n",
    "    accum_recall = 0\n",
    "    accum_map = 0\n",
    "    \n",
    "    num_users = urm_train.shape[0]\n",
    "    \n",
    "    num_users_evaluated = 0\n",
    "    num_users_skipped = 0\n",
    "    \n",
    "    for user_id in range(num_users):\n",
    "        user_profile_start = urm_test.indptr[user_id]\n",
    "        user_profile_end = urm_test.indptr[user_id+1]\n",
    "\n",
    "        relevant_items = urm_test.indices[user_profile_start:user_profile_end]\n",
    "\n",
    "        if relevant_items.size == 0:\n",
    "            num_users_skipped += 1\n",
    "            continue\n",
    "            \n",
    "#         recommendations = recommender.recommend(user_id_array=user_id,\n",
    "#                                                cutoff=recommendation_length,\n",
    "#                                                remove_seen_flag=True\n",
    "#                                                )\n",
    "\n",
    "        expected_ratings = recommender.get_expected_ratings(user_id)\n",
    "        recommended_items = np.flip(np.argsort(expected_ratings), 0)\n",
    "\n",
    "        unseen_items_mask = np.in1d(recommended_items,urm_train[user_id].indices,\n",
    "                                        assume_unique=True, invert=True)\n",
    "\n",
    "        recommendations = recommended_items[unseen_items_mask]\n",
    "\n",
    "\n",
    "        accum_precision += precision(recommendations, relevant_items)\n",
    "        accum_recall += recall(recommendations, relevant_items)\n",
    "        accum_map += mean_average_precision(recommendations, relevant_items)\n",
    "\n",
    "        num_users_evaluated += 1\n",
    "\n",
    "    \n",
    "    accum_precision /= max(num_users_evaluated, 1)\n",
    "    accum_recall /= max(num_users_evaluated, 1)\n",
    "    accum_map /=  max(num_users_evaluated, 1)\n",
    "    \n",
    "    return accum_precision, accum_recall, accum_map, num_users_evaluated, num_users_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params for the recommenders\n",
    "cbf_param = {\n",
    "    \"knn\": 380,\n",
    "    \"shrink\": 9\n",
    "}\n",
    "\n",
    "user_cf_param = {\n",
    "    \"knn\": 249,\n",
    "    \"shrink\": 853\n",
    "}\n",
    "\n",
    "item_cf_param = {\n",
    "    \"knn\": 119,\n",
    "    \"shrink\": 430\n",
    "}\n",
    "\n",
    "slim_bpr_param = {\n",
    "    \"learning_rate\" : 0.05,\n",
    "    \"epochs\": 10,\n",
    "    \"nnz\" : 1,\n",
    "    \"knn\": 200\n",
    "}\n",
    "\n",
    "als_param = {\n",
    "    \"n_factors\": 300,\n",
    "    \"regularization\": 0.15,\n",
    "    \"iterations\": 30\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCF = UserBasedCollaborativeFiltering(urm_train_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCF.fit(knn=user_cf_param[\"knn\"], shrink=user_cf_param[\"shrink\"],similarity='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped = evaluator(userCF, \n",
    "                                                                                            urm_train_validation, \n",
    "                                                                                            urm_test)\n",
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemCF = ItemBasedCollaborativeFiltering(urm_train_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemCF.fit(knn=item_cf_param[\"knn\"], shrink=item_cf_param[\"shrink\"], similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped = evaluator(itemCF, \n",
    "                                                                                            urm_train_validation, \n",
    "                                                                                            urm_test)\n",
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SlimBpr=SlimBPRRec(learning_rate=slim_bpr_param[\"learning_rate\"], epochs=slim_bpr_param[\"epochs\"], nnz=slim_bpr_param[\"nnz\"], knn=slim_bpr_param[\"knn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SlimBpr.fit(urm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped = evaluator(SlimBpr, \n",
    "                                                                                            urm_train_validation, \n",
    "                                                                                            urm_test)\n",
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_elastic = SLIMElasticNetRecommender(urm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_elastic.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped = evaluator(slim_elastic, \n",
    "                                                                                            urm_train_validation, \n",
    "                                                                                            urm_test)\n",
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbf = ContentBasedFiltering(urm_train_validation,ICM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbf.fit(knn=cbf_param[\"knn\"],shrink=cbf_param[\"shrink\"],similarity='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped = evaluator(cbf, \n",
    "                                                                                            urm_train_validation, \n",
    "                                                                                            urm_test)\n",
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS = AlternatingLeastSquare(urm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS.fit(n_factors=als_param[\"n_factors\"], regularization=als_param[\"regularization\"],iterations=als_param[\"iterations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped = evaluator(ALS, \n",
    "                                                                                            urm_train_validation, \n",
    "                                                                                            urm_test)\n",
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hybrid Recommender Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base.Evaluation.Evaluator import EvaluatorHoldout\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(urm_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(urm_test, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "hyperparameters_range_dictionary = {}\n",
    "hyperparameters_range_dictionary[\"w_user\"] = Integer(0, 1000)\n",
    "hyperparameters_range_dictionary[\"w_item\"] = Integer(0, 1000)\n",
    "hyperparameters_range_dictionary[\"w_cbf\"] = Integer(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ParameterTuning.SearchBayesianSkopt import SearchBayesianSkopt\n",
    "\n",
    "recommender_class = HybridRecommender\n",
    "\n",
    "parameterSearch = SearchBayesianSkopt(recommender_class,\n",
    "                                 evaluator_validation=evaluator_validation,\n",
    "                                 evaluator_test=evaluator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ParameterTuning.SearchAbstractClass import SearchInputRecommenderArgs\n",
    "  \n",
    "recommender_input_args = SearchInputRecommenderArgs(\n",
    "    CONSTRUCTOR_POSITIONAL_ARGS = [urm_train, ICM, userCF, itemCF, cbf],     # For a CBF model simply put [URM_train, ICM_train]\n",
    "    CONSTRUCTOR_KEYWORD_ARGS = {},\n",
    "    FIT_POSITIONAL_ARGS = [user_cf_param, item_cf_param, cbf_param, slim_bpr_param, als_param],\n",
    "    FIT_KEYWORD_ARGS = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_input_args_last_test = SearchInputRecommenderArgs(\n",
    "    CONSTRUCTOR_POSITIONAL_ARGS = [urm_train_validation, ICM, userCF, itemCF, cbf],     # For a CBF model simply put [URM_train_validation, ICM_train]\n",
    "    CONSTRUCTOR_KEYWORD_ARGS = {},\n",
    "    FIT_POSITIONAL_ARGS = [user_cf_param, item_cf_param, cbf_param, slim_bpr_param, als_param],\n",
    "    FIT_KEYWORD_ARGS = {}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_folder_path = \"result_experiments/\"\n",
    "\n",
    "# If directory does not exist, create\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "    \n",
    "n_cases = 300\n",
    "n_random_starts = int(n_cases*0.45)\n",
    "metric_to_optimize = \"MAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameterSearch.search(recommender_input_args,\n",
    "                       recommender_input_args_last_test = recommender_input_args_last_test,\n",
    "                       parameter_search_space = hyperparameters_range_dictionary,\n",
    "                       n_cases = n_cases,\n",
    "                       n_random_starts = n_random_starts,\n",
    "                       save_model = \"last\",\n",
    "                       output_folder_path = output_folder_path,\n",
    "                       output_file_name_root = recommender_class.RECOMMENDER_NAME,\n",
    "                       metric_to_optimize = metric_to_optimize,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataIO(folder_path = output_folder_path)\n",
    "search_metadata = data_loader.load_data(recommender_class.RECOMMENDER_NAME + \"_metadata.zip\")\n",
    "best_parameters = search_metadata[\"hyperparameters_best\"]\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = {\n",
    "    \"user_cf\": 993,\n",
    "    \"item_cf\": 513,\n",
    "    \"cbf\": 44,\n",
    "    \"icm_svd\": 0,\n",
    "    \"als\": 0,\n",
    "    \"slim\": 0,\n",
    "    \"elastic\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = HybridRecommender(urm_train_validation, ICM, userCF, itemCF, cbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recommender.fit(user_cf_param=user_cf_param,item_cf_param=item_cf_param,cbf_param=cbf_param,\n",
    "                slim_param=slim_bpr_param,als_param=als_param, w_user=w[\"user_cf\"], w_item=w[\"item_cf\"], w_cbf=w[\"cbf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped = evaluator(recommender, \n",
    "                                                                                            urm_train_validation, \n",
    "                                                                                            urm_test)\n",
    "accum_precision, accum_recall, accum_map, num_user_evaluated, num_users_skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Vi-dduXaj46"
   },
   "source": [
    "**Submission to competition**\n",
    "\n",
    "This step serves as a similar step that you will perform when preparing a submission to the competition. Specially after you have chosen and trained your recommender.\n",
    "\n",
    "For this step the best suggestion is to select the most-performing configuration obtained in the hyperparameter tuning step and to train the recommender using both the train and validation set. Remember that in the competition you do not have access to the test set.\n",
    "\n",
    "Another consideration is that, due to easier and faster calculations, we replaced the user/item identifiers with new ones in the preprocessing step. For the competition, you are required to generate recommendations using the dataset's original identifiers. Due to this, this step also reverts back the newer identifiers with the ones originally found in the dataset.\n",
    "\n",
    "Last, this step creates a function that writes the recommendations for each user in the same file in a tabular format following this format:\n",
    "\n",
    "csv\n",
    "<user_id>,<item_id_1> <item_id_2> <item_id_3> <item_id_4> <item_id_5> <item_id_6> <item_id_7> <item_id_8> <item_id_9> <item_id_10>\n",
    "Always verify the competitions' submission file model as it might vary from the one we presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcSDk9aMtW4k"
   },
   "outputs": [],
   "source": [
    "def load_goodguys():\n",
    "  return pd.read_csv(\"./data_target_users_test.csv\")\n",
    "goodguys=load_goodguys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHzsuFiqtW4k",
    "outputId": "bc36c459-1cc6-460a-b403-95482cf1508d"
   },
   "outputs": [],
   "source": [
    "goodguys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0b3DwuXrayTA",
    "outputId": "27c6d557-b304-427c-bf2c-8fdc090efe64"
   },
   "outputs": [],
   "source": [
    "users_to_recommend = np.random.choice(goodguys.user_id,size=goodguys.size, replace=False)\n",
    "users_to_recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RurxNTttbI-i"
   },
   "outputs": [],
   "source": [
    "def prepare_submission(users_to_recommend: np.array, urm_train: sp.csr_matrix, recommender: object):\n",
    "    \n",
    "    recommendation_length = 10\n",
    "    submission = []\n",
    "    \n",
    "    for user_id in users_to_recommend :\n",
    "\n",
    "        recommendations = recommender.recommend2(user_id, urm_train, recommendation_length)\n",
    "\n",
    "        \n",
    "        submission.append((user_id, [item_id for item_id in recommendations]))\n",
    "   \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KYlvMDVbLxo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = prepare_submission(users_to_recommend, urm_train_validation, recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJZEjE3CbQTp",
    "outputId": "8e2faf6b-144c-4ba3-ee1a-692290e15d3c"
   },
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hCawP60bVm3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "csv_fname = './submission'\n",
    "csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "def write_submission(submissions):\n",
    "    with open(csv_fname, \"w\") as f:\n",
    "        f.write(f\"user_id,item_list\\n\")\n",
    "        for user_id, items in submissions:\n",
    "            f.write(f\"{user_id},{' '.join([str(item) for item in items])}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ov91al-UbXAQ"
   },
   "outputs": [],
   "source": [
    "write_submission(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RecommenderChallenge.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
